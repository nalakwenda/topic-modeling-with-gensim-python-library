{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4fddd78-4ebb-4bfb-8ec2-cfb3313072da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# NLTK\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a80fa4-93b7-4545-8534-034301f78974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c7b87d9-f278-4a97-af9a-44e48b711a6b",
   "metadata": {},
   "source": [
    "## Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd6bb5e-3095-403d-a2e8-797fe0e5704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('./JEOPARDY_QUESTIONS1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef970bd3-01f8-489c-93ff-db4f49554f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>air_date</th>\n",
       "      <th>question</th>\n",
       "      <th>value</th>\n",
       "      <th>answer</th>\n",
       "      <th>round</th>\n",
       "      <th>show_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HISTORY</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'For the last 8 years of his life, Galileo was...</td>\n",
       "      <td>$200</td>\n",
       "      <td>Copernicus</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'No. 2: 1912 Olympian; football star at Carlis...</td>\n",
       "      <td>$200</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'The city of Yuma in this state has a record a...</td>\n",
       "      <td>$200</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'In 1963, live on \"The Art Linkletter Show\", t...</td>\n",
       "      <td>$200</td>\n",
       "      <td>McDonald\\'s</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'Signer of the Dec. of Indep., framer of the C...</td>\n",
       "      <td>$200</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          category    air_date  \\\n",
       "0                          HISTORY  2004-12-31   \n",
       "1  ESPN's TOP 10 ALL-TIME ATHLETES  2004-12-31   \n",
       "2      EVERYBODY TALKS ABOUT IT...  2004-12-31   \n",
       "3                 THE COMPANY LINE  2004-12-31   \n",
       "4              EPITAPHS & TRIBUTES  2004-12-31   \n",
       "\n",
       "                                            question value       answer  \\\n",
       "0  'For the last 8 years of his life, Galileo was...  $200   Copernicus   \n",
       "1  'No. 2: 1912 Olympian; football star at Carlis...  $200   Jim Thorpe   \n",
       "2  'The city of Yuma in this state has a record a...  $200      Arizona   \n",
       "3  'In 1963, live on \"The Art Linkletter Show\", t...  $200  McDonald\\'s   \n",
       "4  'Signer of the Dec. of Indep., framer of the C...  $200   John Adams   \n",
       "\n",
       "       round  show_number  \n",
       "0  Jeopardy!         4680  \n",
       "1  Jeopardy!         4680  \n",
       "2  Jeopardy!         4680  \n",
       "3  Jeopardy!         4680  \n",
       "4  Jeopardy!         4680  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76f8e8c-32d7-49c8-8b35-b621d641c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de09018f-5108-4dfe-b8ac-63d4fc306471",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf1955-ef4c-4052-900f-5512e7e87d89",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4076575-63af-4b33-bba4-0c4334c5d0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['no', 'olympian', 'football', 'star', 'at', 'carlisle', 'indian', 'school', 'mlb', 'seasons', 'with', 'the', 'reds', 'giants', 'braves'], ['the', 'city', 'of', 'yuma', 'in', 'this', 'state', 'has', 'record', 'average', 'of', 'hours', 'of', 'sunshine', 'each', 'year']]\n"
     ]
    }
   ],
   "source": [
    "def tokenization(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "tokenized_sentences = list(tokenization(questions))\n",
    "\n",
    "print(tokenized_sentences [1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b322a47-3ac5-4302-bc47-5d093737d0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'the', 'last', 'years', 'of', 'his', 'life', 'galileo', 'was', 'under', 'house_arrest', 'for', 'espousing', 'this', 'man', 'theory']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(tokenized_sentences, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[tokenized_sentences], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[tokenized_sentences[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a50008-2d93-4781-8bcd-0f8343804963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9ae2d03-88a7-4a6b-a607-fd23ae03a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "tokenized_without_stopwords = remove_stopwords(tokenized_sentences)\n",
    "\n",
    "# Form Bigrams\n",
    "tokenized_without_stopwords_bigrams = make_bigrams(tokenized_without_stopwords)\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "lemmatized_words = lemmatization(tokenized_without_stopwords_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d4c5d74-6f34-4b95-bc5c-bc3b93708152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['olympian', 'season', 'red', 'giant', 'brave'], ['city', 'state', 'record', 'average', 'hour', 'sunshine', 'year'], ['live', 'art', 'linkletter', 'show', 'company', 'serve', 'billionth', 'burger'], ['second', 'president', 'united_state'], ['title', 'aesop_fable', 'insect', 'share', 'billing', 'grasshopper'], ['build', 'still', 'use', 'today'], ['steal', 'baron', 'steal', 'bull'], ['winter', 'record', 'inch', 'snow', 'fall', 'ranger', 'station', 'state']]\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_words[1:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e34b040-8043-403e-98d2-582ccd10ea64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "dictionary = corpora.Dictionary(lemmatized_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts = lemmatized_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd85a00f-1acc-40e5-acb2-3cc5754d2148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "dictionary = corpora.Dictionary(lemmatized_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts = lemmatized_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8ccbb16-0104-4d4d-b9a6-98491a62051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,  id2word=dictionary,num_topics= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16dc8ac8-3af0-40e3-a943-6ae2778ddceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 1 \n",
      "Words: ['river', 'state', 'die', 'type', 'president', 'year', 'serve', 'part', 'make', 'first', 'use', 'mile', 'long', 'man', 'water', 'say', 'get', 'body', 'back', 'start', 'live', 'actor', 'land', 'good', 'foot', 'become', 'go', 'form', 'line', 'study', 'car', 'tree', 'bird', 'take', 'order', 'look', 'eye', 'plant', 'begin', 'earth', 'run', 'house', 'ride', 'find', 'time', 'send', 'add', 'grow', 'food', 'letter']\n",
      "Topic: 2 \n",
      "Words: ['name', 'first', 'bear', 'title', 'write', 'last', 'woman', 'novel', 'play', 'book', 'film', 'man', 'become', 'know', 'make', 'set', 'author', 'call', 'little', 'character', 'work', 'begin', 'base', 'team', 'child', 'wear', 'black', 'classic', 'brother', 'well', 'also', 'red', 'blue', 'include', 'sell', 'famous', 'young', 'wife', 'family', 'use', 'join', 'late', 'story', 'publish', 'king', 'star', 'son', 'english', 'nickname', 'rule']\n",
      "Topic: 3 \n",
      "Words: ['name', 'word', 'play', 'mean', 'come', 'country', 'year', 'film', 'make', 'song', 'first', 'use', 'old', 'tv', 'term', 'know', 'war', 'title', 'show', 'movie', 'star', 'good', 'french', 'also', 'letter', 'call', 'lose', 'feature', 'get', 'battle', 'give', 'see', 'city', 'man', 'state', 'say', 'end', 'refer', 'museum', 'role', 'marry', 'include', 'way', 'american', 'latin', 'host', 'series', 'new', 'fight', 'sound']\n",
      "Topic: 4 \n",
      "Words: ['say', 'capital', 'go', 'hit', 'name', 'call', 'love', 'city', 'make', 'follow', 'give', 'first', 'get', 'tell', 'man', 'record', 'want', 'write', 'never', 'kill', 'know', 'time', 'hear', 'cause', 'ask', 'night', 'take', 'world', 'year', 'top', 'leave', 'meet', 'real', 'break', 'work', 'state', 'guy', 'room', 'country', 'let', 'musical', 'story', 'russian', 'blood', 'home', 'group', 'people', 'animal', 'singer', 'heart']\n",
      "Topic: 5 \n",
      "Words: ['br', 'city', 'large', 'day', 'world', 'island', 'call', 'name', 'use', 'country', 'find', 'number', 'high', 'great', 'build', 'time', 'state', 'member', 'people', 'hold', 'thing', 'make', 'type', 'ancient', 'need', 'part', 'stand', 'one', 'also', 'know', 'area', 'live', 'small', 'group', 'main', 'see', 'include', 'show', 'close', 'say', 'sea', 'open', 'get', 'site', 'big', 'side', 'first', 'many', 'event', 'year']\n",
      "Topic: 6 \n",
      "Words: ['com', 'medium', 'archive', 'www', 'http', 'jpg', 'target', 'see', 'clue_crew', 'href', 'report', 'show', 'college', 'read', 'mp', 'direct', 'sister', 'wmv', 'deliver', 'sitcom', 'gold', 'catch', 'do', 'type', 'clue', 'use', 'stand', 'name', 'hear', 'career', 'hang', 'monitor', 'milk', 'painting', 'cheryl', 'give', 'practice', 'look', 'tool', 'photo', 'blow', 'picture', 'portrait', 'sarah', 'appropriately', 'studio', 'muscle', 'work', 'fine', 'seal']\n"
     ]
    }
   ],
   "source": [
    "doc_lda = lda_model[corpus]\n",
    "for index, topics in lda_model.show_topics(formatted=False, num_words=50):\n",
    "        print('Topic: {} \\nWords: {}'.format(index+1,[w[0] for w in topics] ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "560d7996-4764-4b00-8c0b-5fa59bd2083c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of topic 1 is 63.63515257835388 percent\n",
      "The proportion of topic 2 is 3.2477542757987976 percent\n",
      "The proportion of topic 3 is 3.2475531101226807 percent\n",
      "The proportion of topic 4 is 3.247946873307228 percent\n",
      "The proportion of topic 5 is 23.373456299304962 percent\n",
      "The proportion of topic 6 is 3.2481368631124496 percent\n"
     ]
    }
   ],
   "source": [
    "report=\"This chilled leek & potato soup is traditionally topped with chopped chives\"\n",
    "report = report.split(\" \")\n",
    "\n",
    "for index, proportion in lda_model[dictionary.doc2bow(report)]:\n",
    "    print('The proportion of topic {} is {} percent'.format(index+1, proportion*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c302876-cdda-48de-a228-97f56b1c1217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
